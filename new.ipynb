{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a base neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNeuralNetwork, self).__init__()\n",
    "        # Flatten inputs\n",
    "        #self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(26,416), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = BaseNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNeuralNetwork, self).__init__()\n",
    "        # Flatten inputs\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(28 * 28, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512,256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64,32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32,16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16,10)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = BaseNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, prin = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation (always in three steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0 and prin:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "    return 100*(1-correct)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1-----------------\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.563955\n",
      "Epoch 2-----------------\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.470518\n",
      "Epoch 3-----------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.433033\n",
      "Epoch 4-----------------\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.406194\n",
      "Epoch 5-----------------\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.389415\n",
      "Epoch 6-----------------\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.375461\n",
      "Epoch 7-----------------\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.370210\n",
      "Epoch 8-----------------\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.382567\n",
      "Epoch 9-----------------\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.367546\n",
      "Epoch 10-----------------\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.360022\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "error = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}-----------------\")\n",
    "    #Use train_loop and test_loop functions\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    x = test_loop(test_dataloader, model, loss_fn)\n",
    "    error.append(x)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNeuralNetworkRS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNeuralNetworkRS, self).__init__()\n",
    "        # Flatten inputs\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.start = nn.Linear(28 * 28, 512)\n",
    "        self.lin1 = nn.Linear(512,256)\n",
    "        self.lin2 = nn.Linear(256,128)\n",
    "        self.lin3 = nn.Linear(128,64)\n",
    "        self.lin4 = nn.Linear(64,32)\n",
    "        self.lin5 = nn.Linear(32,16)\n",
    "        self.fc = nn.Linear(16,10)\n",
    "        \n",
    "    def proj(self, x, size):\n",
    "        indice = np.linspace(0, size - 1, size).tolist()\n",
    "        indice = torch.tensor([int(x) for x in indice])\n",
    "        \n",
    "        return torch.index_select(x, 1, indice)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        f = 0\n",
    "        x = self.relu(self.start(x)) + f*self.proj(x, 512)\n",
    "        x = self.relu(self.lin1(x)) + f*self.proj(x, 256)\n",
    "        x = self.relu(self.lin2(x)) + f*self.proj(x, 128)\n",
    "        x = self.relu(self.lin3(x)) + f*self.proj(x, 64)\n",
    "        x = self.relu(self.lin4(x)) + f*self.proj(x, 32)\n",
    "        x = self.relu(self.lin5(x)) + f*self.proj(x, 16)\n",
    "        x = self.relu(self.fc(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "modelRS = BaseNeuralNetworkRS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1-----------------\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.782022\n",
      "Epoch 2-----------------\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.782022\n",
      "Epoch 3-----------------\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.782022\n",
      "Epoch 4-----------------\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.782022\n",
      "Epoch 5-----------------\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.782022\n",
      "Epoch 6-----------------\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.782022\n",
      "Epoch 7-----------------\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.782022\n",
      "Epoch 8-----------------\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.782022\n",
      "Epoch 9-----------------\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.782022\n",
      "Epoch 10-----------------\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.782022\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "error = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}-----------------\")\n",
    "    #Use train_loop and test_loop functions\n",
    "    train_loop(train_dataloader, modelRS, loss_fn, optimizer)\n",
    "    x = test_loop(test_dataloader, model, loss_fn)\n",
    "    error.append(x)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.750000000000007, 17.830000000000002, 15.239999999999998, 14.459999999999996, 14.359999999999996, 13.81, 13.770000000000005, 13.090000000000002, 12.9, 12.490000000000002]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "error_wo = copy.deepcopy(error)\n",
    "\n",
    "plt.plot(error_wo, label = \"w/o RS\")\n",
    "plt.legend()\n",
    "plt.title(\"% error per batch\")\n",
    "plt.ylim((0, 40))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error_without = [24.750000000000007, 17.830000000000002, 15.239999999999998, 14.459999999999996, 14.359999999999996, 13.81, 13.770000000000005, 13.090000000000002, 12.9, 12.490000000000002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU(inplace=True)(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = nn.ReLU(inplace=True)(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avg_pool = nn.AvgPool2d(4)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU(inplace=True)(self.bn(self.conv(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(ResidualBlock, [3, 4, 6, 3])\n",
    "\n",
    "model = ResNet50()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1-----------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x256 and 512x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9cecd50c7e52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}-----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#Use train_loop and test_loop functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-a08f9e7f6707>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, prin)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# Compute prediction and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-c66b1484c663>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x256 and 512x10)"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}-----------------\")\n",
    "    #Use train_loop and test_loop functions\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "940e34ce3f8e8fda083b4ce6ce11a66042581662800fc131ee85ee96d7946fe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
